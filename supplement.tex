%\documentclass{article}
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}

\DeclareMathOperator{\nmd}{NMD}
\DeclareMathOperator{\rnod}{RNOD}

\title{supplement.tex}
\author{Anonymous}
\date{\today}

\addtocounter{table}{2} % align with paper table numbers

\begin{document}

\section{Existing OQ methods from quantification literature}
\label{sec:existingmethods}

For completeness, we introduce the existing OQ methods, which appear in our experiments. Both of these methods do not address ordinality through regularization, like we suggest, but through binary decompositions of the codeframe.

\subsection{Ordinal Quantification Tree (OQT)}
\label{sec:OQT}

\noindent The algorithm trains a
quantifier by arranging probabilistic binary classifiers (one for each
possible bipartition of the ordered set of classes) into an
\emph{ordinal quantification tree} (OQT), which is conceptually
similar to a hierarchical classifier. Two characteristic aspects of
training an OQT are that (a) the loss function used for splitting a
node is a quantification loss (and not a classification loss), e.g.,
the Kullback-Leibler Divergence, and (b) the splitting criterion is
informed by the class order. Given a test document, one generates a
posterior probability for each of the classes by having the document
descend all branches of the trained tree; after this is done for all
documents in the test sample, the PCC multiclass (i.e., non-ordinal)
quantification method is invoked in order to compute the final
prevalence estimates.

The OQT method was only tested in the SemEval 2016 ``Sentiment
analysis in Twitter'' shared task.  While OQT was
the best performer in that subtask, its true value still has to be
assessed, since the above-mentioned subtask evaluated participating
algorithms on one test sample only. Therefore, we
have tested OQT in a much more robust way.

\subsection{Adjusted Regress and Count (ARC)}
\label{sec:ARC}

\noindent The algorithm is similar to OQT in that it trains a hierarchical classifier where the leaves of the tree are the classes, these leaves are ordered left-to-right, and each internal node partitions an ordered sequence of classes in two such subsequences. One difference between the two algorithms is the criterion used in order to decide where to split a given sequence of classes, which for OQT is based on a quantification loss (KLD), and for ARC is based on the principle of minimizing the imbalance (in terms of the number of training examples) of the two subsequences. A second difference is that, once the tree is trained and used to classify the test documents, OQT uses what is basically a PCC algorithm, while ARC uses the ACC multiclass quantification method.

Concerning the quality of
ARC, the same considerations made for OQT apply, since ARC, like OQT,
has only been tested in the Ordinal Quantification subtask of the
SemEval 2016 ``Sentiment analysis in Twitter'' shared task; despite
the fact that it worked well in that context, the experiments that we
are presenting are more conclusive.

\section{Extended results}

\noindent The following results complete the experiments we have shown
in the main paper.

\subsection{Performance in terms of RNOD}

\noindent We have repeated all of our experiments in terms of the \emph{Root Normalised
Order-aware Divergence} (RNOD) evaluation measure, instead of NMD, as proposed
in~\cite{Sakai:2018cf} and as defined as
%
\begin{align}
  \begin{split}
    \label{eq:RNOD}
    \rnod(p,\hat{p}) = & \left(\frac{\sum_{y_{i}\in\mathcal{Y}^{*}}
      \sum_{y_{j}\in\mathcal{Y}}d(y_{j},y_{i})(p(y_{j})-\hat{p}(y_{j}))^{2}}{|\mathcal{Y}^{*}|(n-1)}\right)^{\frac{1}{2}}
  \end{split}
\end{align}
% 
\noindent where
$\mathcal{Y}^{*}=\{y_{i}\in\mathcal{Y}|p(y_{i})>0\}$.

From examining the RNOD results from Tab.~\ref{tab:main_rnod}, we may note that, while some methods change
positions in the ranking, as compared to their ranks in terms of NMD,
general conclusions from the NMD evaluation also hold in terms of
RNOD.

\begin{table}
  \centering
  \caption{Average performance in terms of RNOD (lower is better), in analogy to the NMD results from Tab.~2. For each data set (\textsc{Amazon-OQ-BK} and \textsc{FACT-OQ}), we present the results of the two protocols APP and \mbox{APP-OQ}. The best performance in each column is highlighted in boldface. We further highlight all methods which are not significantly different from the best method, as according to a Wilcoxon signed rank test with $p=0.01$.}
  \label{tab:main_rnod}
  \scriptsize
  \input{jl/res/tex/main_rnod.tex}
\end{table}

We do not choose $\rnod$ as the main evaluation function (and prefer
$\nmd$ for the main paper instead) because we do not think $\rnod$ is
a satisfactory measure for OQ. The reason why we do not consider
$\rnod$ a satisfactory OQ measure is that, without (we think) reason,
it penalises more heavily mistakes (i.e., ``transfers'' of probability
mass from a class to another) closer to the extremes of the
codeframe. For instance, given
$\mathcal{Y}=\{y_{1},y_{3},y_{3}, y_{4},y_{5}\}$, assume
$p=(0.2,0.2,0.2,0.2,0.2)$, and assume two predicted distributions
$\hat{p}'=(0.2,0.2,0.3,0.1,0.2)$ and
$\hat{p}''=(0.2,0.2,0.2,0.3,0.1)$. The two predicted distributions
make essentially the same mistake, i.e., erroneously ``transfer'' a
probability mass of 0.1 from a class $y_{i}$ to a class $y_{(i-1)}$,
the difference being that in $\hat{p}'$ it is the case that $i=4$ and
in $\hat{p}''$ it is the case that $i=5$. According to our intuitions,
$\hat{p}'$ and $\hat{p}''$ should be equally penalised. While NMD
indeed penalises them equally (since
$\nmd(p,\hat{p}')=\nmd(p,\hat{p}'')=0.1$), $\rnod$ does not (since
$\rnod(p,\hat{p}')\approx 0.077$ while
$\rnod(p,\hat{p}'')\approx 0.092$). Sakai \cite{Sakai:2021lp} has proposed
other OQ evaluation measures, such as \emph{Root Symmetric Normalised
Order-aware Divergence} (RSNOD) and \emph{Root Normalised Average
Distance-Weighted sum of squares} (RNADW), but we do not consider them
here since they are variants of RNOD that suffer anyway from the
problem mentioned above.



% -------------------------------------------------------------------

% \begin{table}
%   \centering
%   \caption{NMD (Tab.~2 in the main paper).}
%   \label{main_nmd}
%   \input{jl/res/tex/main_nmd.tex}
% \end{table}

% \begin{table}
%   \centering
%   \caption{RNOD for the experiment from Tab.~2.}
%   \input{jl/res/tex/main_rnod.tex}
% \end{table}

\subsection{Results on other data sets}

We have repeated our experiment from Tab.~2 also several other data sets.

First, we employ a different representation of the \textsc{Amazon-OQ-BK} data, namely a TFIDF representation instead of the RoBERTa embeddings we employ in the main paper. The results with this representation, both in terms of NMD and RNOD, are presented in Tab.~\ref{tab:tfidf}.

Second, we evaluate on a collection of 4 public data sets from the UCI repository and OpenML. To this end, we have first selected regression data sets with at least 30\,000 items. From there on, we have tried to find an equidistant binning which produces at least 10 bins (= ordered classes), each of which have at least 1000 items. We only maintain data sets for which such a binning was possible and we remove all items that lie outside the 10 equidistant bins. In order to maintain as many samples as possible, we maximize the distance between the left-most and right-most bin boundaries. If less then 30\,000 items remain, we omit the data set. From this protocol, we obtain the 4 data sets \textsc{Uci-blog-feedback-OQ}, \textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ}, and \textsc{OpenMl-fried-OQ}. We present the results obtained with these data sets in terms of NMD, see Tab.~\ref{tab:main_others_nmd}, and in terms of RNOD, see Tab.~\ref{tab:main_others_rnod}.

\begin{table}
  \centering
  \caption{NMD (left) and RNOD (right) on a TFIDF representation, instead of RoBERTa embeddings, of the \textsc{Amazon-OQ-BK} data set.}
  \label{tab:tfidf}
  \scriptsize
  \begin{minipage}{.49\textwidth}
    \input{jl/res/tex/main_tfidf_nmd.tex}
  \end{minipage}%
  \begin{minipage}{.49\textwidth}
    \input{jl/res/tex/main_tfidf_rnod.tex}
  \end{minipage}
\end{table}

\begin{table}
  \centering
  \caption{NMD in additional datasets}
  \label{tab:main_others_nmd}
  \resizebox{\textwidth}{!}{
    \input{jl/res/tex/main_others_nmd.tex}
  }%
\end{table}

\begin{table}
  \centering
  \caption{RNOD in additional datasets}
  \label{tab:main_others_rnod}
  \resizebox{\textwidth}{!}{
    \input{jl/res/tex/main_others_rnod.tex}
  }%
\end{table}


\subsection{Hyper-parameter grids}

In our experiments, each method has the opportunity to optimize its hyper-parameters on the APP (or APP-OQ) validation samples. These hyper-pa\-ra\-me\-ters consist of parameters of the quantifier and of parameters of the classifier, with which the quantifier is equipped. After taking out preliminary experiments, which we omit here for conciseness, we have chosen different hyper-parameter grids for the different data sets.

To this end, Tab.~\ref{tab:hyperparameter-roberta-classifier} and Tab.~\ref{tab:hyperparameter-roberta-quantifier} present the parameters for the \textsc{Amazon-OQ-BK} data set. For instance, CC and PCC can choose between 10 hyper-parameter configurations of the classifier (2 class weights $\times$ 5 regularization parameters), but they do not have additional parameters on the quantification level. We note that an inspection of the validation results revealed that the fraction of hold-out data does not considerably affect the results of ACC, PACC, OQT, and ARC. Therefore, we save computational resources by omitting some of the values of this parameter in the final hyper-parameter grid.

Tab.~\ref{tab:hyperparameter-fact-classifier} and Tab.~\ref{tab:hyperparameter-fact-quantifier} present the parameters for the \textsc{Fact-OQ} data. For consiseness, they also contain also the parameters for the UCI and OpenML data sets. The remaining parameters for the UCI and OpenML data sets are presented in Tab.~\ref{tab:hyperparameter-others}

\begin{table}
  \centering
  \caption{Hyper-parameter grid of classifiers when analyzing the \textsc{Amazon-OQ-BK} data in the experiment from Tab.~2.}
  \label{tab:hyperparameter-roberta-classifier}
  \footnotesize
  \begin{tabular}{lll}
    \toprule
    classifier & parameter & values \\
    \midrule
    logistic regression & class weight & \{balanced, unbalanced \} \\
    & regularization parameter $C$ & $\{0.001, 0.01, 0.1, 1.0, 10.0\}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Hyper-parameter grid of quantification methods when analyzing the \textsc{Amazon-OQ-BK} data in the experiment from Tab.~2.}
  \label{tab:hyperparameter-roberta-quantifier}
  \footnotesize
  \begin{tabular}{lll}
    \toprule
    method & parameter & values \\
    \midrule
    CC & no parameters & \\
    PCC & no parameters & \\
    ACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
    PACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
    SLD & no parameters & \\
    \midrule
    OQT & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
    ARC & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
    RUN & $\tau$ & $\{$3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-6$\}$ \\
    IBU & order of polynomial & $\{0, 1, 2\}$ \\
        & interpolation factor & $\{$3e-1, 1e-1, 3e-2, 1e-2, 3e-3, 1e-3$\}$ \\
    \midrule
    o-ACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}\}$ \\
          & $\tau$ & $\{$1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 1e-5, 1e-6, 1e-9$\}$ \\
    o-PACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}\}$ \\
           & $\tau$ & $\{$1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 1e-5, 1e-6, 1e-9$\}$ \\
    o-SLD & order of polynomial & $\{0, 1, 2\}$ \\
        & interpolation factor & $\{$1e-1, 3e-2, 1e-2, 3e-3, 1e-3$\}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Hyper-parameter grid of classifiers when analyzing the \textsc{Fact-OQ} data in the experiment from Tab.~2.}
  \label{tab:hyperparameter-fact-classifier}
  \footnotesize
  \begin{tabular}{lll}
    \toprule
    classifier & parameter & values \\
    \midrule
    probability-calibrated decision tree & class weight & \{balanced, unbalanced\} \\
    & split criterion & \{Gini index, Entropy\} \\
    & maximum depth & $\{4, 6, 8, 10, 12\}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Hyper-parameter grid of quantification methods when analyzing the \textsc{Fact-OQ} data in the experiment from Tab.~2 or any of the data sets \textsc{Uci-blog-feedback-OQ}, \textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ}, and \textsc{OpenMl-fried-OQ}.}
  \label{tab:hyperparameter-fact-quantifier}
  \footnotesize
  \begin{tabular}{lll}
    \toprule
    method & parameter & values \\
    \midrule
    CC & no parameters & \\
    PCC & no parameters & \\
    ACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
    PACC & fraction of hold-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
    SLD & no parameters & \\
    \midrule
    OQT & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
    ARC & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
    RUN & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
        & number of leaf nodes & $\{60, 120, 180\}$ \\
    IBU & order of polynomial & $\{0, 1, 2\}$ \\
        & interpolation factor & $\{$0.1, 0.01, 0.0$\}$ \\
        & number of leaf nodes & $\{60, 120, 180\}$ \\
    \midrule
    o-ACC & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
          & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
    o-PACC & fraction of hold-out data & $\{\frac{1}{3}\}$ \\
           & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
    o-SLD & order of polynomial & $\{0, 1, 2\}$ \\
        & interpolation factor & $\{$1e-1, 3e-2, 1e-2$\}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Hyper-parameter grid of classifiers when analyzing any of the data sets \textsc{Uci-blog-feedback-OQ}, \textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ}, and \textsc{OpenMl-fried-OQ}.}
  \label{tab:hyperparameter-others}
  \footnotesize
  \begin{tabular}{lll}
    \toprule
    classifier & parameter & values \\
    \midrule
    probability-calibrated decision tree & class weight & \{balanced, unbalanced\} \\
    & split criterion & \{Gini index, Entropy\} \\
    & maximum depth & $\{4, 6, 8, 10, 12\}$ \\
    logistic regression & class weight & \{balanced, unbalanced\} \\
    & regularization parameter $C$ & $\{0.001, 0.01, 0.1, 1.0, 10.0\}$ \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Performance in other APP plausibility levels}

\noindent Our APP-OQ protocol selects the 20\% of validation and test samples which we deem most plausible. For completeness, we include here the results for other plausibility levels, which are the second-most, the third-most, the fourth-most, and the least plausible 20\%. In other words: we have divided all APP samples in terms of their conceived plausibility into five levels, the first of which makes our APP-OQ, and we have evaluated all methods in all of these plausibility levels.

As another matter of making our results transparent, we present these tables in a different way, which also includes the hyper-parameters that each method has chosen on the validation samples. Since we also include the regular APP in this mode of presentation, we have 6 tables per data set, i.e. regular APP and five plausibility levels. These tables only consider NMD, but the LaTeX sources of the RNOD tables are part of our supplementary material.

\newcommand{\rankingtable}[2]{
  \begin{table}
    \centering
    \caption{#2}
    \scriptsize
    \input{#1}
  \end{table}
}
% the main data sets

\def \captionamazon {NMD on \textsc{Amazon-OQ-BK}}
\rankingtable{jl/res/tex/amazon_roberta_nmd_all.tex}{\captionamazon, regular APP}
\rankingtable{jl/res/tex/amazon_roberta_nmd_1.tex}{\captionamazon, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/amazon_roberta_nmd_2.tex}{\captionamazon, level 2 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_3.tex}{\captionamazon, level 3 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_4.tex}{\captionamazon, level 4 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_5.tex}{\captionamazon, level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Fact-OQ}}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_all.tex}{\captionfact, regular APP}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_1.tex}{\captionfact, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_2.tex}{\captionfact, level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_3.tex}{\captionfact, level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_4.tex}{\captionfact, level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_5.tex}{\captionfact, level 5 out of 5 (the least smooth)}

% additional data sets

\def \captionamazon {NMD on \textsc{Amazon-OQ-BK}, in an alternative TFIDF representation}
\rankingtable{jl/res/tex/amazon_roberta_nmd_all.tex}{\captionamazon, regular APP}
\rankingtable{jl/res/tex/amazon_roberta_nmd_1.tex}{\captionamazon, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/amazon_roberta_nmd_2.tex}{\captionamazon, level 2 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_3.tex}{\captionamazon, level 3 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_4.tex}{\captionamazon, level 4 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_5.tex}{\captionamazon, level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Uci-blog-feedback-OQ}}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_all.tex}{\captionfact, regular APP}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_1.tex}{\captionfact, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_2.tex}{\captionfact, level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_3.tex}{\captionfact, level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_4.tex}{\captionfact, level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_5.tex}{\captionfact, level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Uci-online-news-popularity-OQ}}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_all.tex}{\captionfact, regular APP}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_1.tex}{\captionfact, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_2.tex}{\captionfact, level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_3.tex}{\captionfact, level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_4.tex}{\captionfact, level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_5.tex}{\captionfact, level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{OpenMl-Yolanda-OQ}}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_all.tex}{\captionfact, regular APP}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_1.tex}{\captionfact, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_2.tex}{\captionfact, level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_3.tex}{\captionfact, level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_4.tex}{\captionfact, level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_5.tex}{\captionfact, level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{OpenMl-fried-OQ}}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_all.tex}{\captionfact, regular APP}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_1.tex}{\captionfact, APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_2.tex}{\captionfact, level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_3.tex}{\captionfact, level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_4.tex}{\captionfact, level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_5.tex}{\captionfact, level 5 out of 5 (the least smooth)}

%\newpage

\end{document}
