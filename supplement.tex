%\documentclass{article}
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{graphicx}

\DeclareMathOperator{\nmd}{NMD}
\DeclareMathOperator{\rnod}{RNOD}

\title{supplement.tex}
\author{Anonymous}
\date{\today}

\addtocounter{table}{2} % align with paper table numbers

\begin{document}

\section{Existing OQ methods from quantification literature}
\label{sec:existingmethods}

\noindent For completeness, we introduce the OQ methods OQT\footnote{Da San Martino, G., Gao, W., Sebastiani, F.: Ordinal text quantification. In: SI- 480 GIR. pp. 937–940 (2016)}
%by~\cite{DaSanMartino:2016jk} 
and ARC,\footnote{Esuli, A.: ISTI-CNR at SemEval-2016 Task 4: Quantification on an ordinal scale. 485 In: SEMEVAL. pp. 92–95 (2016)}
%by~\cite{Esuli:2016lq}, 
which appear
in our main experiments.
%from Section~\ref{sec:results}. 
None of these
methods address ordinality through regularization, like we instead propose, but through binary decompositions of the codeframe.

% \mirkoscomment{These methods are already known in quantification
% literature, we do not suggest improvements for them, and they
% perform poorly in the experiments. Also, the necessity to include
% OQT and ARC in the experiments is clear from the scope of the
% paper. I suggest we move this section to the Experiments
% ``Baselines'' section to give more focus here to our actual
% contributions: i) an introduction to physics methods for the
% quantification audience and ii) our new proposals.}
% \fabsebcomment{Yes, I was thinking too that this section is a good
% candidate for removal, exactly for the reasons you suggest.}
% \mirkoscomment{I have moved them here, to the appendix, for
% completeness.}

% --------------------------------------------------------------------

\subsection{Ordinal Quantification Tree (OQT)}
%~\cite{DaSanMartino:2016jk}
\label{sec:OQT}

\noindent The algorithm OQT
%by~\cite{DaSanMartino:2016jk} 
trains a
quantifier by arranging probabilistic binary classifiers (one for each
possible bipartition of the ordered set of classes) into an
\emph{ordinal quantification tree} (OQT), which is conceptually
similar to a hierarchical classifier. Two characteristic aspects of
training an OQT are that (a) the loss function used for splitting a
node is a quantification loss (and not a classification loss), e.g.,
the Kullback-Leibler Divergence, and (b) the splitting criterion is
informed by the class order. Given a test document, one generates a
posterior probability for each of the classes by having the document
descend all branches of the trained tree; after this is done for all
documents in the test sample, the probabilistic classify-and-count
(PCC) %~\cite{Bella:2010kx} 
multiclass (i.e., non-ordinal)
quantification method is invoked in order to compute the final
prevalence estimates.

The OQT method was only tested in the SemEval 2016 ``Sentiment
analysis in Twitter'' shared task.\footnote{Nakov, P., Ritter, A., Rosenthal, S., Sebastiani, F., Stoyanov, V.: SemEval-2016 514
Task 4: Sentiment analysis in Twitter. In: SEMEVAL. pp. 1–18 (2016)}
%~\cite{Nakov:2016ty}. 
While OQT was
the best performer in that subtask, its true value still has to be
assessed, since the above-mentioned subtask evaluated participating
algorithms on one test sample only. In %Section~\ref{sec:experiments} 
our experiments,
we
have tested OQT in a much more robust way.
% \fabsebcomment{As the binary learner, we probably want a decision
% tree for the astrophysics data and a logistic regressor for the
% textual data.} \mirkoscomment{Yes, this is already the case: OQT
% and ARC are already tested on LRs and SVMs for textual data and on
% DTs for the astrophysics data.}

% --------------------------------------------------------------------

\subsection{Adjusted Regress and Count (ARC)}
%~\cite{Esuli:2016lq}
\label{sec:ARC}

\noindent The algorithm ARC
%by~\cite{Esuli:2016lq} 
is similar to OQT in
that it trains a hierarchical classifier where the leaves of the tree
are the classes, these leaves are ordered left-to-right, and each
internal node partitions an ordered sequence of classes in two such
subsequences. One difference between the two algorithms is the
criterion used in order to decide where to split a given sequence of
classes, which for OQT is based on a quantification loss (KLD), and
for ARC is based on the principle of minimizing the imbalance (in
terms of the number of training examples) of the two subsequences. A
second difference is that, once the tree is trained and used to
classify the test documents, OQT uses what is basically a PCC
algorithm, while ARC uses the adjusted classify-and-count (ACC)
multiclass quantification method.%~\cite{Forman:2005fk}.
%\mirkoscomment{This is the only time we cite this article of Forman;
%can we replace it by Forman:2005fk to save one reference?}

Concerning the quality of ARC, the same considerations made for OQT
apply, since ARC, like OQT, has only been tested in the Ordinal
Quantification subtask of the SemEval 2016 ``Sentiment analysis in
Twitter'' shared task; despite the fact that it worked well in that
context, the experiments that we are presenting in
%Section~\ref{sec:experiments} 
our paper
are more conclusive.

% -------------------------------------------------------------------

\section{Extended results}

\noindent The following results complete the experiments we have shown
in the main paper.

% -------------------------------------------------------------------

% \noindent Table~\ref{tab:amazon_nmd_2}--Table~\ref{tab:amazon_nmd_5}
% display the performances of quantifiers in the buckets 2--5 of our
% APP sample partition. The first and most plausible bucket is already
% presented in Table~\ref{tab:amazon_nmd_1} in the main paper, the
% entire APP without partitioning in Table~\ref{tab:amazon_nmd_all}.

% The five buckets allow us to assess the range of problems in which
% our proposed methods excel. NACC, for instance, beats the non-smooth
% ACC in every single bucket. However, the magnitude of the
% improvement diminishes as the problems become less smooth. Since the
% advantage of regularisation diminishes with decreasingly smooth
% problems, we can also observe that the hyperparameter optimisation
% tends to choose smaller regularisation strengths $\tau$ in less
% smooth buckets. s-SLD wins against the non-smooth SLD in buckets
% 1--3, but looses in the least smooth buckets 4 and 5. ARC, OQT, CC,
% PCC, and the physics-inspired methods consistently provide the least
% accurate results in all buckets.

% \begin{table}[]
% \centering
% \caption{Average performance in the second-smoothest bucket, in
% terms of NMD (lower is better).}
% \scriptsize \input{jl/res/tex/amazon_nmd_2}
% \label{tab:amazon_nmd_2}
% \end{table}

% \begin{table}[]
% \centering
% \caption{Average performance in the third-smoothest bucket, in
% terms of NMD (lower is better).}
% \scriptsize \input{jl/res/tex/amazon_nmd_3}
% \label{tab:amazon_nmd_3}
% \end{table}

% \begin{table}[]
% \centering
% \caption{Average performance in the fourth-smoothest bucket, in
% terms of NMD (lower is better).}
% \scriptsize \input{jl/res/tex/amazon_nmd_4}
% \label{tab:amazon_nmd_4}
% \end{table}

% \begin{table}[]
% \centering
% \caption{Average performance in the least smooth bucket, in terms
% of NMD (lower is better).}
% \scriptsize \input{jl/res/tex/amazon_nmd_5}
% \label{tab:amazon_nmd_5}
% \end{table}

% \pagebreak

% -------------------------------------------------------------------

\subsection{Performance in terms of RNOD}

\noindent We have repeated all of our experiments in terms of the
\emph{Root Normalised Order-aware Divergence} (RNOD) evaluation
measure, instead of NMD, as proposed
by Sakai\footnote{Sakai, T.: Comparing two binned probability distributions for information access 530 evaluation. In: SIGIR. pp. 1073–1076 (2018) }
%in~\cite{Sakai:2018cf} 
and 
defined as
%
\begin{align}
 \begin{split}
 \label{eq:RNOD}
 \rnod(p,\hat{p}) = & \left(\frac{\sum_{y_{i}\in\mathcal{Y}^{*}}
 \sum_{y_{j}\in\mathcal{Y}}d(y_{j},y_{i})(p(y_{j})-\hat{p}(y_{j}))^{2}}{|\mathcal{Y}^{*}|(n-1)}\right)^{\frac{1}{2}}
 \end{split}
\end{align}
% 
\noindent where $\mathcal{Y}^{*}=\{y_{i}\in\mathcal{Y}|p(y_{i})>0\}$. Note that by adopting RNOD we are not simply replacing the evaluation measure, but also the criterion for model selection. That is to say, we have re-run all experiments, this time optimizing hyperparameters for RNOD in place of NMD.

From examining the RNOD results from Table~\ref{tab:main_rnod}, we may
note that, while some methods change positions in the ranking, as
compared to their ranks in terms of NMD, general conclusions from the
NMD evaluation also hold in terms of RNOD.

\begin{table}
 \centering
 \caption{Average performance in terms of RNOD (lower is better), in
 analogy to the NMD results from Table~2. For each data set
 (\textsc{Amazon-OQ-BK} and \textsc{FACT-OQ}), we present the results
 of the two protocols APP and \mbox{APP-OQ}. The best performance in
 each column is highlighted in boldface. We further highlight all
 methods which are not significantly different from the best method,
 as according to a Wilcoxon signed rank test with $p=0.01$.}
 \label{tab:main_rnod}
 \scriptsize \input{jl/res/tex/main_rnod.tex}
\end{table}

We do not choose $\rnod$ as the main evaluation function (and prefer
$\nmd$ for the main paper instead) because we do not think $\rnod$ is
a satisfactory measure for OQ. The reason why we do not consider
$\rnod$ a satisfactory OQ measure is that, without (we think) reason,
it penalises more heavily mistakes (i.e., ``transfers'' of probability
mass from a class to another) closer to the extremes of the
codeframe. For instance, given
$\mathcal{Y}=\{y_{1},y_{3},y_{3}, y_{4},y_{5}\}$, assume
$p=(0.2,0.2,0.2,0.2,0.2)$, and assume two predicted distributions
$\hat{p}'=(0.2,0.2,0.3,0.1,0.2)$ and
$\hat{p}''=(0.2,0.2,0.2,0.3,0.1)$. The two predicted distributions
make essentially the same mistake, i.e., erroneously ``transfer'' a
probability mass of 0.1 from a class $y_{i}$ to a class $y_{(i-1)}$,
the difference being that in $\hat{p}'$ it is the case that $i=4$ and
in $\hat{p}''$ it is the case that $i=5$. According to our intuitions,
$\hat{p}'$ and $\hat{p}''$ should be equally penalised. While NMD
indeed penalises them equally (since
$\nmd(p,\hat{p}')=\nmd(p,\hat{p}'')=0.1$), $\rnod$ does not (since
$\rnod(p,\hat{p}')\approx 0.077$ while
$\rnod(p,\hat{p}'')\approx 0.092$). %\mirkoscomment{I consider the next sentence as a candidate for removal, because it is only in the supplement and removing it could save us one reference.}~\cite{Sakai:2021lp} has proposed 

Other OQ evaluation measures are proposed by Sakai\footnote{Sakai, T.: A closer look at evaluation measures for ordinal quantification. In: CIKM 595
2021 Workshop on Learning to Quantify (2021)}, such
as \emph{Root Symmetric Normalised Order-aware Divergence} (RSNOD) and
\emph{Root Normalised Average Distance-Weighted sum of squares}
(RNADW), but we do not consider them here since they are variants of
RNOD that suffer anyway from the problem mentioned above.

% -------------------------------------------------------------------

\subsection{Results on other data sets}

\noindent We have repeated our experiment from Table~2 using also several
other data sets.

First, we employ a different representation of the
\textsc{Amazon-OQ-BK} data, namely a TFIDF representation\footnote{We extract all uni- and bi-grams appearing at least 5 times in the training set. We use the logarithmic variant of the term-frequency factor, i.e., we compute the term-frequency as $1 + \log(tf)$.}
instead of
the RoBERTa embeddings we employ in the main paper. The aim of this experiment is to better understand how the quality of the representations (RoBERTa representations are assumed to be much more meaningful than TFIDF) affect the results. The results with
this representation, both in terms of NMD and RNOD, are presented in
Table~\ref{tab:tfidf}.
These results show that RoBERTa yields much better representations. Having a wider margin for improvement allows the ordered variants (o-ACC, o-PACC, o-SLD) to exhibit a more marked improvement with respect to the non-ordered variants (ACC, PACC, SLD); this is specially evident in the case of PACC.

Second, we evaluate on a collection of 4 public data sets from the UCI
repository and OpenML. To this end, we have first selected regression
data sets with at least 30\,000 items. From there on, we have tried to
find an equidistant binning which produces at least 10 bins (i.e., ordered
classes), each of which having at least 1000 items. We only retain
data sets for which such a binning was possible and we remove all
items that lie outside the 10 equidistant bins. In order to retain
as many samples as possible, we maximize the distance between the
left-most and right-most bin boundaries. If less then 30\,000 items
remain, we discard the data set. From this protocol, we obtain the 4 data
sets \textsc{Uci-blog-feedback-OQ},
\textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ},
and \textsc{OpenMl-fried-OQ}. We present the results obtained with
these data sets in terms of NMD, see Table~\ref{tab:main_others_nmd},
and in terms of RNOD, see Table~\ref{tab:main_others_rnod}.
These results confirm our main conclusions: the ordered modifications consistently improve over the original (non-ordered) versions, with o-PACC and IBU often displaying the best overall results.

\begin{table}
 \centering
 \caption{NMD (left) and RNOD (right) on a TFIDF representation,
 instead of RoBERTa embeddings, of the \textsc{Amazon-OQ-BK} data
 set.}
 \label{tab:tfidf}
 \scriptsize
 \begin{minipage}{.49\textwidth}
 \input{jl/res/tex/main_tfidf_nmd.tex}
 \end{minipage}%
 \begin{minipage}{.49\textwidth}
 \input{jl/res/tex/main_tfidf_rnod.tex}
 \end{minipage}
\end{table}

\begin{table}
 \centering
 \caption{NMD in additional datasets}
 \label{tab:main_others_nmd}
 \resizebox{\textwidth}{!}{ \input{jl/res/tex/main_others_nmd.tex} }%
\end{table}

\begin{table}
 \centering
 \caption{RNOD in additional datasets}
 \label{tab:main_others_rnod}
 \resizebox{\textwidth}{!}{ \input{jl/res/tex/main_others_rnod.tex} }%
\end{table}

% -------------------------------------------------------------------

\subsection{Hyperparameter grids}

\noindent In our experiments, each method has the opportunity to
optimize its hyperparameters on the APP and on the APP-OQ validation
samples. These hyper\-pa\-ra\-me\-ters consist of parameters of the
quantifier and of parameters of the classifier, with which the
quantifier is equipped. After taking out preliminary experiments,
which we omit here for conciseness, we have chosen different
hyperparameter grids for the different data sets.

To this end, Table~\ref{tab:hyperparameter-roberta-classifier} and
Table~\ref{tab:hyperparameter-roberta-quantifier} present the
parameters for the \textsc{Amazon-OQ-BK} data set. For instance, CC
and PCC can choose between 10 hyperparameter configurations of the
classifier (2 class weights $\times$ 5 regularization parameters), but
they do not have additional parameters on the quantification level. We
note that an inspection of the validation results revealed that the
fraction of held-out data does not considerably affect the results of
ACC, PACC, OQT, and ARC. Therefore, for OQT and ARC we decided to fix the proportion of the held-out split to 1/3 and do not include this hyperparameter in the exploration, since those methods are computationally expensive.% save computational resources by omitting some values of this parameter in the final hyperparameter grid.

Table~\ref{tab:hyperparameter-fact-classifier} and
Table~\ref{tab:hyperparameter-fact-quantifier} present the parameters
for the \textsc{Fact-OQ} data. For conciseness, they also contain the
parameters for the UCI and OpenML data sets. The remaining parameters
for the UCI and OpenML data sets are presented in
Table~\ref{tab:hyperparameter-others}.

\begin{table}
 \centering
 \caption{Hyperparameter grid of classifiers when analyzing the
 \textsc{Amazon-OQ-BK} data in the experiment from Table~2.}
 \label{tab:hyperparameter-roberta-classifier}
 \footnotesize
 \begin{tabular}{lll}
 \toprule
 classifier & parameter & values \\
 \midrule
 logistic regression & class weight & \{balanced, unbalanced \} \\
 & regularization parameter $C$ & $\{0.001, 0.01, 0.1, 1.0, 10.0\}$ \\
 \bottomrule
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Hyperparameter grid of quantification methods when
 analyzing the \textsc{Amazon-OQ-BK} data in the experiment from
 Table~2.}
 \label{tab:hyperparameter-roberta-quantifier}
 \footnotesize
 \begin{tabular}{lll}
 \toprule
 method & parameter & values \\
 \midrule
 CC & no parameters & \\
 PCC & no parameters & \\
 ACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
 PACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
 SLD & no parameters & \\
 \midrule
 OQT & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 ARC & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 RUN & $\tau$ & $\{$3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-6$\}$ \\
 IBU & order of polynomial & $\{0, 1, 2\}$ \\
 & interpolation factor & $\{$3e-1, 1e-1, 3e-2, 1e-2, 3e-3, 1e-3$\}$ \\
 \midrule
 o-ACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}\}$ \\
 & $\tau$ & $\{$1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 1e-5, 1e-6, 1e-9$\}$ \\
 o-PACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}\}$ \\
 & $\tau$ & $\{$1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 1e-5, 1e-6, 1e-9$\}$ \\
 o-SLD & order of polynomial & $\{0, 1, 2\}$ \\
 & interpolation factor & $\{$1e-1, 3e-2, 1e-2, 3e-3, 1e-3$\}$ \\
 \bottomrule
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Hyperparameter grid of classifiers when analyzing the
 \textsc{Fact-OQ} data in the experiment from Table~2.}
 \label{tab:hyperparameter-fact-classifier}
 \footnotesize
 \begin{tabular}{lll}
 \toprule
 classifier & parameter & values \\
 \midrule
 probability-calibrated decision tree & class weight & \{balanced, unbalanced\} \\
 & split criterion & \{Gini index, Entropy\} \\
 & maximum depth & $\{4, 6, 8, 10, 12\}$ \\
 \bottomrule
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Hyperparameter grid of quantification methods when
 analyzing the \textsc{Fact-OQ} data in the experiment from Table~2 or
 any of the data sets \textsc{Uci-blog-feedback-OQ},
 \textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ},
 and \textsc{OpenMl-fried-OQ}.}
 \label{tab:hyperparameter-fact-quantifier}
 \footnotesize
 \begin{tabular}{lll}
 \toprule
 method & parameter & values \\
 \midrule
 CC & no parameters & \\
 PCC & no parameters & \\
 ACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
 PACC & fraction of held-out data & $\{\frac{1}{4}, \frac{1}{3}, \frac{1}{2}\}$ \\
 SLD & no parameters & \\
 \midrule
 OQT & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 ARC & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 RUN & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
 & number of leaf nodes & $\{60, 120, 180\}$ \\
 IBU & order of polynomial & $\{0, 1, 2\}$ \\
 & interpolation factor & $\{$0.1, 0.01, 0.0$\}$ \\
 & number of leaf nodes & $\{60, 120, 180\}$ \\
 \midrule
 o-ACC & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
 o-PACC & fraction of held-out data & $\{\frac{1}{3}\}$ \\
 & $\tau$ & $\{$1e-1, 1e-3, 1e-5$\}$ \\
 o-SLD & order of polynomial & $\{0, 1, 2\}$ \\
 & interpolation factor & $\{$1e-1, 3e-2, 1e-2$\}$ \\
 \bottomrule
 \end{tabular}
\end{table}

\begin{table}
 \centering
 \caption{Hyperparameter grid of classifiers when analyzing any of
 the data sets \textsc{Uci-blog-feedback-OQ},
 \textsc{Uci-online-news-popularity-OQ}, \textsc{OpenMl-Yolanda-OQ},
 and \textsc{OpenMl-fried-OQ}.}
 \label{tab:hyperparameter-others}
 \footnotesize
 \begin{tabular}{lll}
 \toprule
 classifier & parameter & values \\
 \midrule
 probability-calibrated decision tree & class weight & \{balanced, unbalanced\} \\
 & split criterion & \{Gini index, Entropy\} \\
 & maximum depth & $\{4, 6, 8, 10, 12\}$ \\
 logistic regression & class weight & \{balanced, unbalanced\} \\
 & regularization parameter $C$ & $\{0.001, 0.01, 0.1, 1.0, 10.0\}$ \\
 \bottomrule
 \end{tabular}
\end{table}

% --------------------------------------------------------------------

\subsection{Performance in other APP plausibility levels}

\noindent Our APP-OQ protocol selects the 20\% of validation and test
samples which we deem most plausible. For completeness, we include
here the results for other plausibility levels, which are the
second-most, the third-most, the fourth-most, and the least plausible
20\%. In other words: we have divided all APP samples in terms of
their conceived plausibility into five levels, the first of which
makes our APP-OQ, and we have evaluated all methods in all of these
plausibility levels. Recall that every evaluation entails optimizing the hyperparameters on the corresponding validation split. Recall also that every split concerns only the validation and the test data, while the training set is the same in all cases.

In order to make our experimentation more transparent, we report the results accompaigned by the hyperparameters
that each method has chosen on the validation samples. Since we also
include the regular APP in this mode of presentation, we have 6 tables
per data set, i.e., regular APP and five plausibility levels. These
tables only consider NMD, but the LaTeX sources of the RNOD tables are made available as part of our supplementary material as well.
Often, the regularization hyperparameters favouring smoother solutions are stronger in the smoothest cases. 
%See, e.g., o-SLD for \textsc{Amazon-OQ-BK}, that chooses $o=2$ (a second order polynomial) with strength $i=0.01$ for the highest level of smoothing, and with strength $i=0.001$ for the second level; to then choose 

\newcommand{\rankingtable}[2]{
\begin{table}
 \centering
 \caption{#2}
 \scriptsize \input{#1}
\end{table}
}
% the main data sets

\def \captionamazon {NMD on \textsc{Amazon-OQ-BK}}
\rankingtable{jl/res/tex/amazon_roberta_nmd_all.tex}{\captionamazon, regular
APP} 
\rankingtable{jl/res/tex/amazon_roberta_nmd_1.tex}{\captionamazon,
APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/amazon_roberta_nmd_2.tex}{\captionamazon, level 2
out of 5} 
\rankingtable{jl/res/tex/amazon_roberta_nmd_3.tex}{\captionamazon,
level 3 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_4.tex}{\captionamazon, level 4
out of 5} 
\rankingtable{jl/res/tex/amazon_roberta_nmd_5.tex}{\captionamazon,
level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Fact-OQ}}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_all.tex}{\captionfact, regular
APP} 
\rankingtable{jl/res/tex/dirichlet_fact_nmd_1.tex}{\captionfact, APP-OQ
= level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_2.tex}{\captionfact, level 2 out
of 5} 
\rankingtable{jl/res/tex/dirichlet_fact_nmd_3.tex}{\captionfact, level
3 out of 5} 
\rankingtable{jl/res/tex/dirichlet_fact_nmd_4.tex}{\captionfact,
level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_fact_nmd_5.tex}{\captionfact, level 5 out
of 5 (the least smooth)}

% additional data sets

\def \captionamazon {NMD on \textsc{Amazon-OQ-BK}, in an alternative
TFIDF representation}
\rankingtable{jl/res/tex/amazon_roberta_nmd_all.tex}{\captionamazon, regular
APP} \rankingtable{jl/res/tex/amazon_roberta_nmd_1.tex}{\captionamazon,
APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/amazon_roberta_nmd_2.tex}{\captionamazon, level 2
out of 5} \rankingtable{jl/res/tex/amazon_roberta_nmd_3.tex}{\captionamazon,
level 3 out of 5}
\rankingtable{jl/res/tex/amazon_roberta_nmd_4.tex}{\captionamazon, level 4
out of 5} \rankingtable{jl/res/tex/amazon_roberta_nmd_5.tex}{\captionamazon,
level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Uci-blog-feedback-OQ}}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_all.tex}{\captionfact,
regular APP}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_1.tex}{\captionfact,
APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_2.tex}{\captionfact,
level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_3.tex}{\captionfact,
level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_4.tex}{\captionfact,
level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_blog-feedback_nmd_5.tex}{\captionfact,
level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{Uci-online-news-popularity-OQ}}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_all.tex}{\captionfact,
regular APP}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_1.tex}{\captionfact,
APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_2.tex}{\captionfact,
level 2 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_3.tex}{\captionfact,
level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_4.tex}{\captionfact,
level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_online-news-popularity_nmd_5.tex}{\captionfact,
level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{OpenMl-Yolanda-OQ}}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_all.tex}{\captionfact, regular
APP} \rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_1.tex}{\captionfact,
APP-OQ = level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_2.tex}{\captionfact, level 2
out of 5} \rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_3.tex}{\captionfact,
level 3 out of 5}
\rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_4.tex}{\captionfact, level 4
out of 5} \rankingtable{jl/res/tex/dirichlet_Yolanda_nmd_5.tex}{\captionfact,
level 5 out of 5 (the least smooth)}

\def \captionfact{NMD on \textsc{OpenMl-fried-OQ}}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_all.tex}{\captionfact, regular
APP} \rankingtable{jl/res/tex/dirichlet_fried_nmd_1.tex}{\captionfact, APP-OQ
= level 1 out of 5 (the smoothest)}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_2.tex}{\captionfact, level 2 out
of 5} \rankingtable{jl/res/tex/dirichlet_fried_nmd_3.tex}{\captionfact, level
3 out of 5} \rankingtable{jl/res/tex/dirichlet_fried_nmd_4.tex}{\captionfact,
level 4 out of 5}
\rankingtable{jl/res/tex/dirichlet_fried_nmd_5.tex}{\captionfact, level 5 out
of 5 (the least smooth)}

%\newpage

\end{document}
